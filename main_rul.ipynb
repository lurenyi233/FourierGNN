{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f3c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20204120\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples:  torch.Size([17631, 30, 14]) torch.Size([17631, 1, 1])\n",
      "test samples:  torch.Size([63, 30, 14]) torch.Size([63, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "current_system = platform.system()\n",
    "if current_system == \"Linux\":\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    new_directory = \"/enc/y_song/work/pytorch_geometric_temporal/\"\n",
    "    os.chdir(new_directory)\n",
    "    print(\"current_directory:\", new_directory)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from pylab import *\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "\n",
    "\n",
    "# train数据读取#######################################\n",
    "train_df = pd.read_csv('train_FD001.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                    's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                    's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "train_df = train_df.sort_values(['id', 'cycle'])\n",
    "# train数据读取#######################################\n",
    "\n",
    "# test 数据读取#######################################\n",
    "test_df = pd.read_csv('test_FD001.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "# test 数据读取#######################################\n",
    "\n",
    "# lable数据读取#######################################\n",
    "truth_df = pd.read_csv('RUL_FD001.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "# lable数据读取#######################################\n",
    "\n",
    "# train数据处理#######################################\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "# 获得各个id下cycle的最大值，shape（100，2）\n",
    "rul.columns = ['id', 'max']\n",
    "# 将rul的列抬头由cycle换为max\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "# 根据id这列，将max这列放到train_df的末尾，对于每个id不同cycle，max的值是一直的\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "# 新增一列RUL，用当前的max减去当前的cycle\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "# 新增cycle_norm这列,作为自变量之一\n",
    "cols_normalize = train_df.columns.difference(['id', 'cycle', 'RUL'])\n",
    "# 找出需要标准化的列\n",
    "min_max_scaler = preprocessing.MinMaxScaler((0, 1))\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)\n",
    "# 对需要标准化的列进行标准化\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "# 将没标准化的列和标准化的列合并\n",
    "train_df = join_df.reindex(columns=train_df.columns)\n",
    "# 根据train_df.columns的顺序对各列进行重新排序\n",
    "\n",
    "# 将RUL中大于130的值改为130，cycle_norm和RUL无关，所以不管\n",
    "train_df['RUL'].loc[train_df['RUL'] > 125] = 125\n",
    "# train数据处理#######################################\n",
    "\n",
    "# test 数据处理#######################################\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
    "                            columns=cols_normalize,\n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns=test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "# 将truth_df的RUL一列抬头改为more\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "# 原truth_df的索引为0~99，新增一个id列，其值为1~100\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "# 如第一个零件，test数据中最大运行31个周期，RUL中还有112个周期。故最大周期为143\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "# 将test的最大周期这一列加到test_df中，各id在不同cycle的最大周期一致。\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "# 算得test的实时RUL值\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 删掉用来计算RUL的max这一列。\n",
    "# 将RUL中大于130的值改为130\n",
    "# test_df['RUL'].loc[test_df['RUL'] >125]=125\n",
    "# test 数据处理#######################################\n",
    "#     print(train_df.shape)\n",
    "#     print(test_df.shape)\n",
    "\n",
    "\n",
    "# 将数据格式变为(样本循环次数, 时间窗大小：50, 特征数)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "\n",
    "# 对应数据格式生成标签\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]\n",
    "\n",
    "\n",
    "# 选择特征列\n",
    "# sensor_cols = [ 's2', 's3','s4', 's6', 's7', 's8', 's9', 's11', 's12', 's13', 's14','s15', 's17', 's20', 's21']\n",
    "# sequence_cols = ['setting1', 'setting2', 'cycle_norm']\n",
    "# #     sequence_cols = ['setting1', 'setting2']\n",
    "# sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# https://ieeexplore.ieee.org/document/10146287\n",
    "sequence_cols = ['s2', 's3', 's4', 's7', 's8', 's9', 's11', 's12', 's13', 's14', 's15', 's17', 's20', 's21']\n",
    "# sequence_cols = ['setting1', 'setting2', 'cycle_norm']\n",
    "#     sequence_cols = ['setting1', 'setting2']\n",
    "# sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# ******************************************************************测试集********************************************\n",
    "length_of_test_df = []\n",
    "for id in test_df['id'].unique():\n",
    "    length_of_test = len(test_df[test_df['id'] == id])\n",
    "    length_of_test_df.append(length_of_test)\n",
    "# print(length_of_test_df)\n",
    "id_of_test_df = list(range(1, 101))\n",
    "# print(id_of_test_df)\n",
    "\n",
    "# 按测试集长度大小排序的id号\n",
    "length_of_test_df_sorted, id_of_test_df_sorted = zip(*sorted(zip(length_of_test_df, id_of_test_df)))\n",
    "\n",
    "id_of_test_df_sorted_30 = id_of_test_df_sorted[:12]\n",
    "id_of_test_df_sorted_60 = id_of_test_df_sorted[12:26]\n",
    "id_of_test_df_sorted_90 = id_of_test_df_sorted[26:37]\n",
    "id_of_test_df_sorted_120 = id_of_test_df_sorted[37:]\n",
    "\n",
    "\n",
    "# ******************************************************************测试集********************************************\n",
    "\n",
    "# 数据集类\n",
    "class MyDataFloder(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.input_feature = x\n",
    "        self.input__label = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.input_feature[index]), self.input__label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input__label)\n",
    "\n",
    "\n",
    "BATCH_SIZE_training = 128\n",
    "BATCH_SIZE_test = 128\n",
    "\n",
    "# for sequence_length in range(30,150,30):\n",
    "\n",
    "for sequence_length in range(30, 60, 30):\n",
    "    # 训练数据样本和标签################################\n",
    "    seq_gen = (list(gen_sequence(train_df[train_df['id'] == id], sequence_length, sequence_cols))\n",
    "               for id in train_df['id'].unique())\n",
    "    locals()['training_sample_' + str(sequence_length)] = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "    label_gen = [gen_labels(train_df[train_df['id'] == id], sequence_length, ['RUL'])\n",
    "                 for id in train_df['id'].unique()]\n",
    "    locals()['training_label_' + str(sequence_length)] = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "    locals()['training_sample_' + str(sequence_length)] = locals()['training_sample_' + str(sequence_length)].transpose(\n",
    "        0, 2, 1)\n",
    "    # 训练数据样本和标签################################\n",
    "\n",
    "    # 测试数据样本和标签################################\n",
    "    test_sample = [test_df[test_df['id'] == id][sequence_cols].values[-sequence_length:]\n",
    "                   for id in id_of_test_df_sorted_120]\n",
    "    locals()['test_sample_120_' + str(sequence_length)] = np.asarray(test_sample).astype(np.float32)\n",
    "\n",
    "    test_label = [test_df[test_df['id'] == id]['RUL'].values[-1] for id in id_of_test_df_sorted_120]\n",
    "    locals()['test_label_120_' + str(sequence_length)] = np.array(test_label).astype(np.float32)\n",
    "\n",
    "    locals()['test_sample_120_' + str(sequence_length)] = locals()['test_sample_120_' + str(sequence_length)].transpose(\n",
    "        0, 2, 1)\n",
    "    # 测试数据样本和标签################################\n",
    "\n",
    "    # print(\"训练样本和标签：\", locals()['training_sample_' + str(sequence_length)].shape,\n",
    "    #       locals()['training_label_' + str(sequence_length)].shape)\n",
    "    # print(\"测试样本和标签：\", locals()['test_sample_120_' + str(sequence_length)].shape,\n",
    "    #       locals()['test_label_120_' + str(sequence_length)].shape)\n",
    "    #\n",
    "    # locals()['train_dataset_' + str(sequence_length)] = MyDataFloder(\n",
    "    #     locals()['training_sample_' + str(sequence_length)], locals()['training_label_' + str(sequence_length)])\n",
    "    # locals()['test_dataset_' + str(sequence_length)] = MyDataFloder(locals()['test_sample_120_' + str(sequence_length)],\n",
    "    #                                                                 locals()['test_label_120_' + str(sequence_length)])\n",
    "    #\n",
    "    # locals()['train_loader_' + str(sequence_length)] = torch.utils.data.DataLoader(\n",
    "    #     dataset=locals()['train_dataset_' + str(sequence_length)], batch_size=BATCH_SIZE_training, shuffle=True)\n",
    "    # locals()['test_loader_' + str(sequence_length)] = torch.utils.data.DataLoader(\n",
    "    #     dataset=locals()['test_dataset_' + str(sequence_length)], batch_size=BATCH_SIZE_test, shuffle=False)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # cuda or cpu\n",
    "batch_size = 128\n",
    "\n",
    "train_x_tensor = torch.from_numpy(training_sample_30).permute(0, 2, 1).contiguous().type(torch.FloatTensor).to(DEVICE)\n",
    "train_target_tensor = torch.from_numpy(training_label_30).type(torch.FloatTensor).unsqueeze(2).to(DEVICE)\n",
    "\n",
    "print(\"training samples: \", train_x_tensor.size(), train_target_tensor.size())\n",
    "\n",
    "\n",
    "test_x_tensor = torch.from_numpy(test_sample_120_30).permute(0, 2, 1).contiguous().type(torch.FloatTensor).to(DEVICE)\n",
    "test_target_tensor = torch.from_numpy(test_label_120_30).type(torch.FloatTensor).unsqueeze(1).unsqueeze(2).to(DEVICE)\n",
    "print(\"test samples: \", test_x_tensor.size(), test_target_tensor.size())\n",
    "\n",
    "# training samples:  torch.Size([17631, 14, 1, 30]) torch.Size([17631, 1])\n",
    "# test samples:  torch.Size([63, 14, 1, 30]) torch.Size([63, 1])\n",
    "\n",
    "train_dataset_new = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset_new, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "\n",
    "test_dataset_new = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "val_dataloader = torch.utils.data.DataLoader(test_dataset_new, batch_size=100, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75432af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\20204120\\\\OneDrive - TU Eindhoven\\\\TUe\\\\Jupyter\\\\2021 Learning examples\\\\20240125 FourierGNN', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\python38.zip', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\DLLs', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\lib', 'C:\\\\Users\\\\20204120\\\\Anaconda3', '', 'C:\\\\Users\\\\20204120\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\20204120\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\20204120\\\\.ipython']\n",
      "Training configs: Namespace(batch_size=32, data='RUL', decay_rate=0.5, device='cuda:0', early_stop=False, embed_size=128, exponential_decay_step=5, feature_size=14, hidden_size=256, learning_rate=1e-05, pre_length=1, seq_length=30, train_epochs=100, train_ratio=0.7, val_ratio=0.2, validate_freq=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from data.data_loader import Dataset_ECG, Dataset_Dhfm, Dataset_Solar, Dataset_Wiki\n",
    "from model.FourierGNN import FGN\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "from utils.utils import save_model, load_model, evaluate\n",
    "\n",
    "# main settings can be seen in markdown file (README.md)\n",
    "parser = argparse.ArgumentParser(description='fourier graph network for multivariate time series forecasting')\n",
    "parser.add_argument('--data', type=str, default='RUL', help='data set')\n",
    "parser.add_argument('--feature_size', type=int, default='14', help='feature size')\n",
    "parser.add_argument('--seq_length', type=int, default=30, help='inout length')\n",
    "parser.add_argument('--pre_length', type=int, default=1, help='predict length')\n",
    "parser.add_argument('--embed_size', type=int, default=128, help='hidden dimensions')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, help='hidden dimensions')\n",
    "parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='input data batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00001, help='optimizer learning rate')\n",
    "parser.add_argument('--exponential_decay_step', type=int, default=5)\n",
    "parser.add_argument('--validate_freq', type=int, default=1)\n",
    "parser.add_argument('--early_stop', type=bool, default=False)\n",
    "parser.add_argument('--decay_rate', type=float, default=0.5)\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7)\n",
    "parser.add_argument('--val_ratio', type=float, default=0.2)\n",
    "parser.add_argument('--device', type=str, default='cuda:0', help='device')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "print(f'Training configs: {args}')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FGN(pre_length=args.pre_length, embed_size=args.embed_size, feature_size=args.feature_size, seq_length=args.seq_length, hidden_size=args.hidden_size)\n",
    "my_optim = torch.optim.RMSprop(params=model.parameters(), lr=args.learning_rate, eps=1e-08)\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=args.decay_rate)\n",
    "forecast_loss = nn.MSELoss(reduction='mean').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40c8834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20204120\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([128, 1, 1])) that is different to the input size (torch.Size([128, 14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([128, 30, 14]) torch.Size([128, 1, 1])\n",
      "torch.Size([95, 30, 14]) torch.Size([95, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20204120\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([95, 1, 1])) that is different to the input size (torch.Size([95, 14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW : MAPE 70.425782699%; MAE 46.253551483; RMSE 59.462677002.\n",
      "| end of epoch   0 | time: 129.79s | train_total_loss 7462.3839 | val_loss 3535.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20204120\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([63, 1, 1])) that is different to the input size (torch.Size([63, 14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result_train_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4e3325e00e17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m         print('| end of epoch {:3d} | time: {:5.2f}s | train_total_loss {:5.4f} | val_loss {:5.4f}'.format(\n\u001b[0;32m     78\u001b[0m                 epoch, (time.time() - epoch_start_time), loss_total / cnt, val_loss))\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_train_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result_train_file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def validate(model, vali_loader):\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    loss_total = 0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for i, (x, y) in enumerate(vali_loader):\n",
    "        cnt += 1\n",
    "        y = y.float().to(device)\n",
    "        x = x.float().to(device)\n",
    "        forecast = model(x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        loss = forecast_loss(forecast, y)\n",
    "        loss_total += float(loss)\n",
    "        forecast = forecast.detach().cpu().numpy()  # .squeeze()\n",
    "        y = y.detach().cpu().numpy()  # .squeeze()\n",
    "        preds.append(forecast)\n",
    "        trues.append(y)\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    score = evaluate(trues, preds)\n",
    "    print(f'RAW : MAPE {score[0]:7.9%}; MAE {score[1]:7.9f}; RMSE {score[2]:7.9f}.')\n",
    "    model.train()\n",
    "    return loss_total/cnt\n",
    "\n",
    "def test():\n",
    "    result_test_file = 'output/'+args.data+'/train'\n",
    "    model = load_model(result_test_file, 48)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    sne = []\n",
    "    for index, (x, y) in enumerate(test_dataloader):\n",
    "        y = y.float().to(device)\n",
    "        x = x.float().to(device)\n",
    "        forecast = model(x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        forecast = forecast.detach().cpu().numpy()  # .squeeze()\n",
    "        y = y.detach().cpu().numpy()  # .squeeze()\n",
    "        preds.append(forecast)\n",
    "        trues.append(y)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    score = evaluate(trues, preds)\n",
    "    print(f'RAW : MAPE {score[0]:7.9%}; MAE {score[1]:7.9f}; RMSE {score[2]:7.9f}.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        loss_total = 0\n",
    "        cnt = 0\n",
    "        for index, (x, y) in enumerate(train_dataloader):\n",
    "            cnt += 1\n",
    "            y = y.float().to(device)\n",
    "            x = x.float().to(device)\n",
    "            print(x.size(), y.size())\n",
    "            forecast = model(x)\n",
    "            y = y.permute(0, 2, 1).contiguous()\n",
    "            loss = forecast_loss(forecast, y)\n",
    "            loss.backward()\n",
    "            my_optim.step()\n",
    "            loss_total += float(loss)\n",
    "\n",
    "        if (epoch + 1) % args.exponential_decay_step == 0:\n",
    "            my_lr_scheduler.step()\n",
    "        if (epoch + 1) % args.validate_freq == 0:\n",
    "            val_loss = validate(model, val_dataloader)\n",
    "\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | train_total_loss {:5.4f} | val_loss {:5.4f}'.format(\n",
    "                epoch, (time.time() - epoch_start_time), loss_total / cnt, val_loss))\n",
    "        save_model(model, result_train_file, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea399ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class VariableLengthLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(VariableLengthLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, sequences, lengths):\n",
    "        # sequences: (batch_size, max_seq_length, input_size)\n",
    "        # lengths: list containing the actual lengths of each sequence in the batch\n",
    "\n",
    "        # Sort sequences by length in descending order\n",
    "        sorted_lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
    "        sorted_sequences = sequences[sorted_indices]\n",
    "\n",
    "        # Pack the sequences\n",
    "        packed_sequences = pack_padded_sequence(sorted_sequences, sorted_lengths, batch_first=True)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        packed_output, _ = self.lstm(packed_sequences)\n",
    "\n",
    "        # Unpack the sequences\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Reorder the output to the original order\n",
    "        _, original_indices = torch.sort(sorted_indices)\n",
    "        output = output[original_indices]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(output[:, -1, :])  # Take the output from the last time step\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "model = VariableLengthLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Example input sequences with different lengths\n",
    "batch_size = 3\n",
    "max_seq_length = 5\n",
    "sequences = torch.randn(batch_size, max_seq_length, input_size)\n",
    "\n",
    "# Example lengths of sequences in the batch\n",
    "lengths = torch.tensor([4, 3, 5])\n",
    "\n",
    "# Forward pass\n",
    "output = model(sequences, lengths)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd508d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example input sequences with different lengths\n",
    "batch_size = 3\n",
    "max_seq_length = 6\n",
    "sequences = torch.randn(batch_size, max_seq_length, input_size)\n",
    "\n",
    "# Example lengths of sequences in the batch\n",
    "lengths = torch.tensor([4, 3, 5])\n",
    "\n",
    "# Forward pass\n",
    "output = model(sequences, lengths)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
